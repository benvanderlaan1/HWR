{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52d5cf36",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy\n",
    "import torch\n",
    "import zipfile\n",
    "import evaluate\n",
    "import pandas as pd\n",
    "\n",
    "#import torchvision.transforms as T\n",
    "\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from transformers import TrOCRProcessor\n",
    "from transformers import default_data_collator\n",
    "from transformers import VisionEncoderDecoderModel\n",
    "from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments\n",
    "\n",
    "print(\"Succesfully imported all required packages.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b42ebcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_DATA_TXT_FILE = \"Data/augmented_data.txt\"\n",
    "IMAGE_ZIP_FILE = \"Data/augmented_data.zip\"\n",
    "OUTPUT_DIR = \"Output2\"\n",
    "IMG_DIR = \"Data/img\"\n",
    "\n",
    "if not os.path.exists(OUTPUT_DIR):\n",
    "    os.mkdir(OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78080f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "try:\n",
    "    new = \"\"\n",
    "    with open(IMG_DATA_TXT_FILE) as f:\n",
    "        lines = f.readlines()\n",
    "        for i, line in enumerate(lines):\n",
    "            new += line[:-1]\n",
    "            if i % 3 == 2:\n",
    "                new += \"\\n\"\n",
    "            else:\n",
    "                new += \"\\t\"\n",
    "\n",
    "    with open(IMG_DATA_TXT_FILE, 'w+') as f:\n",
    "        f.write(new)\n",
    "\n",
    "    print(\"Succesfully rewritten test file structure.\")\n",
    "except:\n",
    "    print(\"File does not exists:\", IMG_DATA_TXT_FILE)\n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b443e5ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(IMG_DIR):\n",
    "    try:\n",
    "        with zipfile.ZipFile(IMAGE_ZIP_FILE, 'r') as zip_ref:\n",
    "            zip_ref.extractall(\"Data\")\n",
    "        print(\"Succesfully extracted the images.\")\n",
    "    except:\n",
    "        print(\"Unable to extract:\", IMAGE_ZIP_FILE)\n",
    "else:\n",
    "    print(\"Data/img folder already contains\", len(os.listdir(\"Data/img\")), \"items.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca51529b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load the data\n",
    "\n",
    "print(\"loading data\")\n",
    "\n",
    "_df = pd.read_table(IMG_DATA_TXT_FILE, header=None)\n",
    "_df.rename(columns={0: \"file_name\", 1: \"text\"}, inplace=True)\n",
    "\n",
    "print(\"data loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddd58497",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class IAMDataset(Dataset):\n",
    "    def __init__(self, root_dir, df, processor, max_target_length=128):\n",
    "        self.root_dir = root_dir\n",
    "        self.df = df\n",
    "        self.processor = processor\n",
    "        self.max_target_length = max_target_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        # get file name + text \n",
    "        file_name = self.df['file_name'][idx]\n",
    "        text = self.df['text'][idx]\n",
    "\n",
    "        # some file names end with jp instead of jpg, the two lines below fix this\n",
    "        if file_name.endswith('jp'):\n",
    "            file_name = file_name + 'g'\n",
    "\n",
    "        if file_name.endswith('pn'):\n",
    "            file_name = file_name + 'g'\n",
    "            \n",
    "        if file_name.endswith('augm'):\n",
    "            file_name = file_name + 'ented.png'\n",
    "\n",
    "        # prepare image (i.e. resize + normalize)\n",
    "        image = Image.open(self.root_dir + file_name).convert(\"RGB\")\n",
    "        pixel_values = self.processor(image, return_tensors=\"pt\").pixel_values\n",
    "        \n",
    "        \n",
    "        if (pixel_values.size() != torch.Size([1, 3, 384, 384])):\n",
    "            print(file_name)\n",
    "            print(pixel_values.dim())\n",
    "            print(pixel_values.size())\n",
    "            \n",
    "\n",
    "        # add labels (input_ids) by encoding the text\n",
    "        labels = self.processor.tokenizer(text, \n",
    "                                          padding=\"max_length\", \n",
    "                                          max_length=self.max_target_length).input_ids\n",
    "        \n",
    "        # important: make sure that PAD tokens are ignored by the loss function\n",
    "        labels = [label if label != self.processor.tokenizer.pad_token_id else -100 for label in labels]\n",
    "\n",
    "        encoding = {\"pixel_values\": pixel_values.squeeze(), \"labels\": torch.tensor(labels)}\n",
    "        \n",
    "        if encoding[\"labels\"].size() != torch.Size([128]):\n",
    "            print(encoding[\"labels\"].size())\n",
    "            print(file_name)\n",
    "        \n",
    "        return encoding\n",
    "    \n",
    "print(\"IAMDataset Class definition created.\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "02383c7c",
   "metadata": {},
   "source": [
    "# Split dataset for train/val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc5cc514",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "# we reset the indices to start from zero\n",
    "train_df.reset_index(drop=True, inplace=True)\n",
    "test_df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8e4973c",
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = TrOCRProcessor.from_pretrained(\"microsoft/trocr-base-handwritten\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82c6419d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create train and eval data\n",
    "train_dataset = IAMDataset(root_dir='Data/img/',\n",
    "                          df=train_df,\n",
    "                          processor=processor)\n",
    "eval_dataset = IAMDataset(root_dir='Data/img/',\n",
    "                          df=test_df,\n",
    "                          processor=processor)\n",
    "\n",
    "print(\"Number of training examples:\", len(train_dataset))\n",
    "print(\"Number of validation examples:\", len(eval_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9e238cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check Validity of data.\n",
    "\n",
    "try:\n",
    "    for item in tqdm(eval_dataset):\n",
    "        labels = item['labels']\n",
    "        labels[labels == -100] = processor.tokenizer.pad_token_id\n",
    "        label_str = processor.decode(labels, skip_special_tokens=True)\n",
    "except:\n",
    "    print(\"Cannot find item.\")\n",
    "    \n",
    "\n",
    "try:\n",
    "    for item in tqdm(train_dataset):\n",
    "        labels = item['labels']\n",
    "        labels[labels == -100] = processor.tokenizer.pad_token_id\n",
    "        label_str = processor.decode(labels, skip_special_tokens=True)\n",
    "except:\n",
    "    print(\"Cannot find item.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7fba4a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding = train_dataset[0]\n",
    "\n",
    "labels = encoding['labels']\n",
    "labels[labels == -100] = processor.tokenizer.pad_token_id\n",
    "label_str = processor.decode(labels, skip_special_tokens=True)\n",
    "label_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54b87d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a pretrained base model.\n",
    "model = VisionEncoderDecoderModel.from_pretrained(\"microsoft/trocr-base-stage1\", )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a90e089",
   "metadata": {},
   "outputs": [],
   "source": [
    "processor.tokenizer.all_special_ids\n",
    "len(processor.tokenizer.get_vocab())\n",
    "vocab = processor.tokenizer.get_vocab()\n",
    "SPECIALS = set(processor.tokenizer.all_special_ids)\n",
    "UNK_TOK = processor.tokenizer.unk_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d68e2daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "cer_metric = evaluate.load(\"cer\")\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    labels_ids = pred.label_ids\n",
    "    pred_ids = pred.predictions\n",
    "    \n",
    "    pred_ids[pred_ids == -100] = processor.tokenizer.pad_token_id\n",
    "    \n",
    "    try:                  \n",
    "        pred_str = processor.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "    except Exception as e:\n",
    "        raise Exception(\"BREAK\", labels_ids, pred_ids)\n",
    "    \n",
    "    labels_ids[labels_ids == -100] = processor.tokenizer.pad_token_id\n",
    "    \n",
    "    label_str = processor.batch_decode(labels_ids, skip_special_tokens=True)\n",
    "\n",
    "    cer = cer_metric.compute(predictions=pred_str, references=label_str)\n",
    "    \n",
    "    return {\"cer\": cer}\n",
    "\n",
    "print(\"compute_metrics definition created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdecced2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set special tokens used for creating the decoder_input_ids from the labels\n",
    "model.config.decoder_start_token_id = processor.tokenizer.cls_token_id\n",
    "model.config.pad_token_id = processor.tokenizer.pad_token_id\n",
    "# make sure vocab size is set correctly\n",
    "model.config.vocab_size = model.config.decoder.vocab_size\n",
    "\n",
    "# set beam search parameters\n",
    "model.config.eos_token_id = processor.tokenizer.sep_token_id\n",
    "#model.config.max_length = 64\n",
    "model.config.max_new_tokens = 64\n",
    "model.config.early_stopping = True\n",
    "model.config.no_repeat_ngram_size = 3\n",
    "model.config.length_penalty = 2.0\n",
    "model.config.num_beams = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc53a6e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = Seq2SeqTrainingArguments(\n",
    "    predict_with_generate=True,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    fp16=True, \n",
    "    output_dir=OUTPUT_DIR,\n",
    "    logging_steps=2,\n",
    "    save_steps=1000,\n",
    "    eval_steps=2000,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40419817",
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate trainer\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    tokenizer=processor.image_processor,\n",
    "    args=training_args,\n",
    "    compute_metrics=compute_metrics,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    data_collator=default_data_collator,\n",
    ")\n",
    "\n",
    "\n",
    "print(\"Initialized all training params.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e06f1acb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "865e4168",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(training_args.output_dir)"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "common-cu113.m108",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cu113:m108"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
